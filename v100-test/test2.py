import os
import time
import torch
from diffusers import DiffusionPipeline
import numpy as np
import traceback

# ==============================================================================
# 1. åŸºç¡€é…ç½®
# ==============================================================================
# ä½¿ç”¨å…¨éƒ¨ 8 å¼ å¡ï¼Œæœ‰èµ„æºå°±è¦å……åˆ†åˆ©ç”¨ï¼Œç¡®ä¿ç¨³å®š
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"

# æœ¬åœ°æ¨¡å‹è·¯å¾„
local_model_path = "/storage/v-jinpewang/az_workspace/wenjun/Qwen-Image2/my_hf_cache/Qwen-Image/"

if not os.path.exists(local_model_path):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„: {local_model_path}")

print("ğŸš€ å¯åŠ¨ 8xV100 æé€Ÿç¨³å®šæ–¹æ¡ˆ (FP32 | 512x512)")
print("è¯´æ˜: åˆ©ç”¨ 8 å¡çš„å·¨å¤§æ˜¾å­˜ä¼˜åŠ¿ï¼Œè½»æ¾åŠ è½½å…¨é‡ FP32 æ¨¡å‹ï¼Œå½»åº•æœç»é»‘å›¾å’Œ OOMã€‚")

# ==============================================================================
# 2. åŠ è½½æ¨¡å‹ (å…¨é‡ FP32)
# ==============================================================================
try:
    print("Adding model to GPU memory (this may take a minute)...")
    pipe = DiffusionPipeline.from_pretrained(
        local_model_path,
        torch_dtype=torch.float32,  # <--- ã€æ ¸å¿ƒã€‘å…¨é‡ FP32ï¼ŒV100 çš„å”¯ä¸€è§£
        device_map="balanced",  # <--- 8å¡æ˜¾å­˜è¶³å¤Ÿï¼Œè‡ªåŠ¨å¹³è¡¡å³å¯ï¼Œæ— éœ€å¤æ‚é…ç½®
        use_safetensors=True,
        trust_remote_code=True,
        local_files_only=True
    )

    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼(å…¨é‡é©»ç•™æ˜¾å­˜ï¼Œæ—  CPU Offload)")

    # å¼€å¯ Tiling ä½œä¸ºåŒé‡ä¿é™©ï¼ˆè™½ç„¶ 8 å¡è·‘ 512 å¯èƒ½ä¸éœ€è¦ï¼Œä½†å¼€äº†æ›´ç¨³ï¼‰
    pipe.enable_vae_tiling()

    # --------------------------------------------------------------------------
    # 3. æ¨ç† (ä¸¥æ ¼é™åˆ¶ 512x512)
    # --------------------------------------------------------------------------
    prompt = '''ohwhwj man, wearing a red helmet and orange life jacket, smiling with water droplets on his face.'''
    negative_prompt = " "

    # ã€æŒ‰ä½ è¦æ±‚ã€‘æ°¸ä¹…é”å®š 512 x 512
    width, height = (512, 512)

    print(f"Starting generation ({width}x{height})...")

    if torch.cuda.is_available():
        torch.cuda.synchronize()
    start_time = time.time()

    # ç”Ÿæˆ
    image = pipe(
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=30,
        true_cfg_scale=4.0,
        # Generator æ”¾åœ¨ CPU ä¸Šä»¥ä¿è¯å¤šå¡å…¼å®¹
        generator=torch.Generator(device="cpu").manual_seed(42)
    ).images[0]

    if torch.cuda.is_available():
        torch.cuda.synchronize()
    end_time = time.time()

    # --------------------------------------------------------------------------
    # 4. éªŒè¯ä¸ä¿å­˜
    # --------------------------------------------------------------------------
    img_arr = np.array(image)
    save_path = "output_v100_8gpu_512.png"

    if img_arr.max() == 0:
        # å¦‚æœè¿™é‡Œè¿˜é»‘ï¼Œé‚£çœŸæ˜¯è§é¬¼äº†ï¼Œç¡¬ä»¶æˆ–é©±åŠ¨å¯èƒ½æœ‰å¤§é—®é¢˜
        print("âŒ ç»æœ›äº†ï¼š8å¡ FP32 ä¾ç„¶å…¨é»‘ã€‚è¯·æ£€æŸ¥ CUDA/é©±åŠ¨ç‰ˆæœ¬ã€‚")
    else:
        image.save(save_path)
        print("-" * 30)
        print(f"ğŸ‰ æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")
        print(f"è€—æ—¶: {end_time - start_time:.2f}s (æé€Ÿ)")
        print("âœ… ç»“è®ºï¼šå¯¹äº V100ï¼Œ'8å¡ + FP32' æ˜¯æœ€å®Œç¾çš„è§£å†³æ–¹æ¡ˆã€‚")

except torch.cuda.OutOfMemoryError:
    print("âŒ OOM: éš¾ä»¥ç½®ä¿¡ï¼Œ8å¼ å¡è·‘512è¿˜èƒ½çˆ†æ˜¾å­˜ï¼Ÿ")
    traceback.print_exc()

except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    traceback.print_exc()