import os
import time
import torch
from diffusers import DiffusionPipeline
import numpy as np
import traceback
import shutil

# 1. æ˜¾å¡é…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"

# 2. è·¯å¾„
local_model_path = "/storage/v-jinpewang/az_workspace/wenjun/Qwen-Image2/my_hf_cache/Qwen-Image/"

# 3. å‡†å¤‡ Offload æ–‡ä»¶å¤¹
offload_folder = "./model_offload"
if not os.path.exists(offload_folder):
    os.makedirs(offload_folder, exist_ok=True)

# ==============================================================================
# 4. ã€æ ¸å¿ƒé…ç½®ã€‘FP32 + æ˜¾å­˜é™é¢ (å…³é”®ä¿®å¤)
# ==============================================================================
print("ğŸš€ å¯åŠ¨ 8xV100 ç»ˆææ–¹æ¡ˆ (FP32 + æ˜¾å­˜é™é¢)")

# ã€å…³é”®ã€‘V100 åªæœ‰ 32GBã€‚
# Text Encoder FP32 çº¦ 28GBï¼Œç›´æ¥æ”¾ä¸€å¼ å¡å¿…çˆ†ã€‚
# æˆ‘ä»¬é™åˆ¶æ¯å¼ å¡åªå­˜ 20GB æƒé‡ã€‚
# 8å¼ å¡ x 20GB = 160GB æ€»å®¹é‡ï¼Œè¿œå¤§äºæ¨¡å‹æ€»éœ€çš„ ~110GBï¼Œè¶³å¤Ÿäº†ã€‚
# è¿™æ ·ä¼šå¼ºè¿« Text Encoder å’Œ Transformer è¢«åˆ‡ç¢å‡åŒ€åˆ†å¸ƒï¼Œä¸ä»…ä¸çˆ†æ˜¾å­˜ï¼Œè®¡ç®—ä¹Ÿæ›´å‡è¡¡ã€‚
max_memory_config = {
    0: "20GB", 1: "20GB", 2: "20GB", 3: "20GB",
    4: "20GB", 5: "20GB", 6: "20GB", 7: "20GB",
}

try:
    pipe = DiffusionPipeline.from_pretrained(
        local_model_path,
        torch_dtype=torch.float32,  # ä¿æŒ FP32 é˜²é»‘å›¾
        device_map="balanced",  # é…åˆ max_memory ä½¿ç”¨
        max_memory=max_memory_config,  # <--- ã€ä¿®å¤ OOM çš„å…³é”®ã€‘
        offload_folder=offload_folder,
        use_safetensors=True,
        trust_remote_code=True,
        local_files_only=True
    )

    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼(Text Encoder è¢«æˆåŠŸåˆ‡åˆ†)")

    # å¼€å¯ Tiling
    pipe.enable_vae_tiling()

    # --------------------------------------------------------------------------
    # 5. æ¨ç† (512x512)
    # --------------------------------------------------------------------------
    prompt = '''ohwhwj man, wearing a red helmet and orange life jacket, smiling with water droplets on his face.'''
    negative_prompt = " "
    width, height = (512, 512)

    print(f"Starting generation ({width}x{height})...")

    if torch.cuda.is_available():
        torch.cuda.synchronize()
    start_time = time.time()

    image = pipe(
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=30,
        true_cfg_scale=4.0,
        generator=torch.Generator(device="cpu").manual_seed(42)
    ).images[0]

    if torch.cuda.is_available():
        torch.cuda.synchronize()
    end_time = time.time()

    # --------------------------------------------------------------------------
    # 6. éªŒè¯
    # --------------------------------------------------------------------------
    img_arr = np.array(image)
    save_path = "output_v100_8gpu_fp32_fix.png"

    if img_arr.max() == 0:
        print("âŒ ä¾ç„¶å…¨é»‘ã€‚")
    else:
        image.save(save_path)
        print(f"ğŸ‰ æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")
        print(f"è€—æ—¶: {end_time - start_time:.2f}s")

except torch.cuda.OutOfMemoryError:
    print("âŒ ä¾ç„¶ OOMï¼Ÿ")
    print("è¯·å°è¯•å°† max_memory_config è¿›ä¸€æ­¥è°ƒä½åˆ° '18GB'ã€‚")
    traceback.print_exc()

except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    # å¦‚æœè¿™é‡ŒæŠ¥ 'Expected all tensors on same device'ï¼Œé‚£æ˜¯ diffusers çš„è‡ªåŠ¨åˆ‡åˆ† bug
    # é‡åˆ°é‚£ç§æƒ…å†µå¿…é¡»æ¢ç”¨æˆ‘ä¹‹å‰å†™çš„ 'æ‰‹åŠ¨æµæ°´çº¿è„šæœ¬' (fast_inference_8gpu.py)
    traceback.print_exc()

finally:
    if os.path.exists(offload_folder):
        try:
            shutil.rmtree(offload_folder)
        except:
            pass