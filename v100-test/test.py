import os
import time
import torch
from diffusers import DiffusionPipeline
import numpy as np
import traceback
import shutil

# 1. æ˜¾å¡é…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"

# 2. è·¯å¾„
local_model_path = "/storage/v-jinpewang/az_workspace/wenjun/Qwen-Image2/my_hf_cache/Qwen-Image/"

print("ğŸš€ å¯åŠ¨ V100 ç»ˆææ–¹æ¡ˆ (Sequential Offload ç‰ˆ)")
print("è¯´æ˜ï¼šè¿™å°†è‡ªåŠ¨å¤„ç†å¤šå¡/CPUä¹‹é—´çš„æƒé‡è°ƒåº¦ï¼Œå½»åº•è§£å†³ 'Expected all tensors on same device' é”™è¯¯ã€‚")

try:
    # ==========================================================================
    # 3. åŠ è½½æ¨¡å‹ (FP32)
    # ==========================================================================
    # æ³¨æ„ï¼šè¿™é‡Œ device_map è®¾ä¸º "auto" æˆ–è€…ä¸è®¾ç½®ï¼Œè®©åé¢çš„ enable_sequential_cpu_offload æ¥ç®¡
    # æˆ‘ä»¬å…ˆå°è¯•ä¸è®¾ç½® device_mapï¼Œæ‰‹åŠ¨åŠ è½½åˆ° CPUï¼Œç„¶åå¼€å¯ offload
    pipe = DiffusionPipeline.from_pretrained(
        local_model_path,
        torch_dtype=torch.bfloat16,  # <--- å…¨é‡ FP32 (é˜²é»‘å›¾)
        use_safetensors=True,
        trust_remote_code=True,
        local_files_only=True
        # æ³¨æ„ï¼šè¿™é‡Œç§»é™¤äº† device_map å’Œ max_memoryï¼Œäº¤ç»™ä¸‹é¢çš„ offload å¤„ç†
    )

    print("âœ… æ¨¡å‹å·²åŠ è½½åˆ° CPU (FP32 Mode)")

    # ==========================================================================
    # 4. ã€æ ¸å¿ƒä¿®å¤ã€‘å¼€å¯é¡ºåº CPU å¸è½½ (Sequential Offload)
    # ==========================================================================
    # è¿™ä¸ªåŠŸèƒ½æ˜¯ diffusers çš„â€œå¤§æ‹›â€ã€‚
    # å®ƒä¼šå°†æ¨¡å‹æ‰€æœ‰æ¨¡å—ä¿ç•™åœ¨ CPU ä¸Šï¼Œæ¨ç†æ—¶åªæŠŠå½“å‰éœ€è¦è®¡ç®—çš„ä¸€å±‚ï¼ˆLayerï¼‰åŠ è½½åˆ° GPUã€‚
    # è®¡ç®—å®Œè¿™å±‚ï¼Œç«‹åˆ»é‡Šæ”¾æ˜¾å­˜ã€‚
    # ä¼˜ç‚¹ï¼šæå¤§èŠ‚çœæ˜¾å­˜ï¼ˆV100 32GB è·‘ FP32 æ¯«æ— å‹åŠ›ï¼‰ï¼Œç»å¯¹ä¸ä¼šæŠ¥è®¾å¤‡ä¸ä¸€è‡´é”™è¯¯ã€‚
    # ç¼ºç‚¹ï¼šé€Ÿåº¦ä¼šæ…¢ä¸€äº›ï¼ˆå› ä¸ºæœ‰é¢‘ç¹çš„ PCIe æ•°æ®ä¼ è¾“ï¼‰ï¼Œä½†ä¸ºäº†è·‘é€šï¼Œè¿™æ˜¯å€¼å¾—çš„ã€‚

    # æ³¨æ„ï¼šåœ¨å¤šå¡ç¯å¢ƒä¸‹ï¼Œå®ƒé€šå¸¸ä½¿ç”¨ç¬¬ä¸€å¼ å¯è§å¡ (cuda:0) è¿›è¡Œè®¡ç®—ã€‚
    # å¦‚æœä½ æƒ³åˆ©ç”¨å¤šå¡ï¼Œenable_model_cpu_offload() ä¼šæ›´å¥½ï¼Œä½†åœ¨ V100+FP32 è¿™ç§æé™è¾¹ç¼˜ï¼Œsequential æœ€ç¨³ã€‚
    pipe.enable_sequential_cpu_offload()
    print("âœ… å·²å¼€å¯ Sequential CPU Offload (è§£å†³è®¾å¤‡ä¸ä¸€è‡´ & OOM)")

    # å¼€å¯ Tiling èŠ‚çœæ˜¾å­˜
    pipe.enable_vae_tiling()

    # --------------------------------------------------------------------------
    # 5. æ¨ç† (512x512)
    # --------------------------------------------------------------------------
    prompt = '''ohwhwj man, wearing a red helmet and orange life jacket, smiling with water droplets on his face.'''
    negative_prompt = " "
    width, height = (512, 512)

    print(f"Starting generation ({width}x{height})...")
    print("æç¤ºï¼šç”±äºå¼€å¯äº† Offloadï¼Œæ¨ç†é€Ÿåº¦ä¼šæ¯”çº¯ GPU æ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

    start_time = time.time()

    image = pipe(
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=30,
        true_cfg_scale=4.0,
        generator=torch.Generator(device="cpu").manual_seed(42)
    ).images[0]

    end_time = time.time()

    # --------------------------------------------------------------------------
    # 6. éªŒè¯
    # --------------------------------------------------------------------------
    img_arr = np.array(image)
    save_path = "output_v100_offload_fix.png"

    if img_arr.max() == 0:
        print("âŒ å›¾ç‰‡ä¾ç„¶æ˜¯å…¨é»‘çš„ã€‚")
    else:
        image.save(save_path)
        print(f"ğŸ‰ æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")
        print(f"è€—æ—¶: {end_time - start_time:.2f}s")

except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    traceback.print_exc()