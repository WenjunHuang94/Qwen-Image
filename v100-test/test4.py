import os
import torch
import numpy as np
from PIL import Image
from tqdm.auto import tqdm
import gc

# å¼•å…¥å¿…è¦çš„åº“
from diffusers import AutoencoderKLQwenImage, QwenImageTransformer2DModel, FlowMatchEulerDiscreteScheduler
from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer
from diffusers.pipelines.qwenimage.pipeline_qwenimage import QwenImagePipeline
from accelerate import dispatch_model

# ==============================================================================
# 1. åŸºç¡€é…ç½®
# ==============================================================================
# ä½¿ç”¨å…¨éƒ¨ 8 å¼ å¡
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"

# ä½ çš„æœ¬åœ°è·¯å¾„
local_model_path = "/storage/v-jinpewang/az_workspace/wenjun/Qwen-Image2/my_hf_cache/Qwen-Image/"
dtype = torch.float32  # ã€ç»å¯¹æ ¸å¿ƒã€‘FP32 é˜²æ­¢ V100 é»‘å›¾

print("ğŸš€ å¯åŠ¨ 8xV100 æ‰‹åŠ¨æµæ°´çº¿æ–¹æ¡ˆ (Manual Pipeline)...")
print("   - ç­–ç•¥: æ‰‹åŠ¨å°†æ¨¡å‹æ‹†è§£åˆ° 8 å¼ å¡ï¼Œå½»åº•ç»•è¿‡è‡ªåŠ¨åˆ†é…çš„ OOM å‘ã€‚")

# ==============================================================================
# 2. æ‰‹åŠ¨åŠ è½½ç»„ä»¶ (é€ä¸ªåŠ è½½ï¼Œç²¾å‡†æ§åˆ¶)
# ==============================================================================

# --- A. åŠ è½½ Text Encoder (GPU 0) ---
print("\n1. [GPU 0] Loading Text Encoder (~28GB)...")
# Qwen2.5-VL-7B åœ¨ FP32 ä¸‹å¾ˆå¤§ï¼Œæˆ‘ä»¬è®© GPU 0 åªå¹²è¿™ä¸€ä»¶äº‹
text_encoder = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    local_model_path, subfolder="text_encoder", torch_dtype=dtype
).to("cuda:0")
text_encoder.eval()

tokenizer = Qwen2Tokenizer.from_pretrained(local_model_path, subfolder="tokenizer")

# --- B. åŠ è½½ VAE (GPU 7) ---
print("2. [GPU 7] Loading VAE...")
vae = AutoencoderKLQwenImage.from_pretrained(
    local_model_path, subfolder="vae", torch_dtype=dtype
).to("cuda:7")
vae.eval()
vae.enable_tiling()  # å¿…é¡»å¼€å¯ï¼ŒFP32è§£ç æåƒæ˜¾å­˜

# --- C. åŠ è½½ Transformer (GPU 1-6) ---
print("3. [GPU 1-6] Loading & Sharding Transformer (~80GB)...")
# å…ˆåŠ è½½åˆ° CPUï¼Œé¿å…åˆå§‹åŒ–æ—¶çˆ†æ˜¾å­˜
transformer = QwenImageTransformer2DModel.from_pretrained(
    local_model_path, subfolder="transformer", torch_dtype=dtype
)
transformer.eval()

# --- æ‰‹åŠ¨åˆ‡åˆ† Transformer ---
# è¿™æ˜¯ä¸€ä¸ª 60 å±‚çš„æ·±å±‚ç½‘ç»œã€‚æˆ‘ä»¬æŠŠå®ƒåˆ‡æˆ 6 æ®µï¼Œæ¯æ®µ 10 å±‚ï¼Œåˆ†åˆ«æ”¾åœ¨ GPU 1 åˆ° 6ã€‚
# è¿™æ ·æ¯å¼ å¡åªå  ~13GB æ˜¾å­˜ï¼Œæå…¶å®‰å…¨ã€‚
device_map = {}

# 1. åŸºç¡€å±‚ (Input Projections) æ”¾ GPU 1
for name, _ in transformer.named_children():
    if name != "transformer_blocks":
        device_map[name] = 1  # cuda:1

# 2. åˆ‡åˆ† 60 ä¸ª Block
num_blocks = 60
cards = [1, 2, 3, 4, 5, 6]  # ä½¿ç”¨è¿™6å¼ å¡
layers_per_card = 10  # 60 / 6 = 10

for i in range(num_blocks):
    # è®¡ç®—å½“å‰å±‚åº”è¯¥å»å“ªå¼ å¡ (0-9->card[0], 10-19->card[1]...)
    card_idx = i // layers_per_card
    if card_idx >= len(cards): card_idx = len(cards) - 1
    target_device = cards[card_idx]

    device_map[f"transformer_blocks.{i}"] = target_device

print(f"   - åˆ‡åˆ†è¡¨ç”Ÿæˆå®Œæ¯•: 60å±‚ Block å¹³å‡åˆ†é…åˆ° GPU 1 ~ GPU 6")

# åº”ç”¨åˆ‡åˆ† (ç‰©ç†ç§»åŠ¨æƒé‡)
transformer = dispatch_model(transformer, device_map=device_map)

# å¼ºåˆ¶æ¸…ç†ä¸€ä¸‹ CPU å†…å­˜
gc.collect()
torch.cuda.empty_cache()
print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

# ==============================================================================
# 3. å‡†å¤‡è¾…åŠ© Helper
# ==============================================================================
scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(local_model_path, subfolder="scheduler")
# è¿™ä¸ª pipeline åªæ˜¯ä¸ºäº†å€Ÿç”¨å®ƒçš„ encode_prompt å’Œ pack_latents æ–¹æ³•ï¼Œä¸å ç”¨æ˜¾å­˜
helper_pipe = QwenImagePipeline(
    vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,
    transformer=None, scheduler=scheduler
)

# ==============================================================================
# 4. æ‰‹åŠ¨æ¨ç†å¾ªç¯ (Manual Inference Loop)
# ==============================================================================
prompt = "ohwhwj man, wearing a red helmet and orange life jacket, smiling with water droplets on his face."
negative_prompt = " "
width, height = 512, 512  # ä½ è¦æ±‚çš„ 512
num_inference_steps = 30
guidance_scale = 4.0
seed = 42

print(f"\nğŸ¬ Starting Inference ({width}x{height}) - 30 Steps...")
torch.cuda.synchronize()
import time

start_time = time.time()

with torch.no_grad():
    # --- Step 1: Encode Prompt (åœ¨ GPU 0) ---
    print("   [1/4] Encoding Prompt on GPU 0...")
    prompt_embeds, prompt_mask = helper_pipe.encode_prompt(
        prompt=prompt, device="cuda:0", num_images_per_prompt=1
    )
    neg_embeds, neg_mask = helper_pipe.encode_prompt(
        prompt=negative_prompt, device="cuda:0", num_images_per_prompt=1
    )

    # --- Step 2: Prepare Latents (åœ¨ GPU 1) ---
    print("   [2/4] Preparing Latents on GPU 1...")
    # åˆå§‹å™ªå£°æ”¾åœ¨ Transformer çš„å…¥å£ (GPU 1)
    latents = torch.randn(
        (1, transformer.config.in_channels // 4, height // 16, width // 16),
        device="cuda:1", dtype=dtype, generator=torch.Generator().manual_seed(seed)
    )
    # Pack latents
    latents = helper_pipe._pack_latents(latents, 1, transformer.config.in_channels // 4, height // 16 * 8,
                                        width // 16 * 8)

    # --- Step 3: Denoising Loop (æ•°æ®æµ: GPU 1 -> ... -> GPU 6) ---
    print("   [3/4] Denoising (Pipeline: GPU 1 -> 2 -> 3 -> 4 -> 5 -> 6)...")

    scheduler.set_timesteps(num_inference_steps, device="cuda:1")
    timesteps = scheduler.timesteps

    # å‡†å¤‡è¾…åŠ©å˜é‡ (ç§»åˆ° Transformer å…¥å£ GPU 1)
    # accelerate ä¼šè‡ªåŠ¨å¤„ç†å±‚çº§é—´çš„ä¼ è¾“ï¼Œä½†æˆ‘ä»¬éœ€è¦æŠŠå…¥å£æ•°æ®å–‚ç»™ GPU 1
    prompt_embeds = prompt_embeds.to("cuda:1")
    prompt_mask = prompt_mask.to("cuda:1")
    neg_embeds = neg_embeds.to("cuda:1")
    neg_mask = neg_mask.to("cuda:1")

    img_shapes = [(1, height // 16, width // 16)]
    txt_seq_lens = prompt_mask.sum(dim=1).tolist()
    neg_txt_seq_lens = neg_mask.sum(dim=1).tolist()

    for i, t in enumerate(tqdm(timesteps)):
        # 1. å‡†å¤‡ Step è¾“å…¥ (ç¡®ä¿åœ¨ GPU 1)
        # ä¸Šä¸€æ­¥çš„è¾“å‡ºåœ¨ GPU 6ï¼Œéœ€è¦æ‹‰å› GPU 1
        latent_model_input = latents.to("cuda:1")
        timestep = t.expand(latents.shape[0]).to(dtype).to("cuda:1")

        # 2. Transformer Forward
        # æ­¤æ—¶æ•°æ®ä¼šè‡ªåŠ¨æµè½¬ï¼šGPU 1 -> 2 -> 3 -> 4 -> 5 -> 6

        # Positive
        noise_pred_cond = transformer(
            hidden_states=latent_model_input,
            timestep=timestep / 1000,
            encoder_hidden_states=prompt_embeds,
            encoder_hidden_states_mask=prompt_mask,
            img_shapes=img_shapes,
            txt_seq_lens=txt_seq_lens,
            return_dict=False
        )[0]  # æœ€ç»ˆç»“æœåœ¨ GPU 6

        # Negative
        noise_pred_uncond = transformer(
            hidden_states=latent_model_input,
            timestep=timestep / 1000,
            encoder_hidden_states=neg_embeds,
            encoder_hidden_states_mask=neg_mask,
            img_shapes=img_shapes,
            txt_seq_lens=neg_txt_seq_lens,
            return_dict=False
        )[0]  # æœ€ç»ˆç»“æœåœ¨ GPU 6

        # 3. Guidance & Step (åœ¨ GPU 6 è¿›è¡Œ)
        # è¿™äº›è®¡ç®—é‡å¾ˆå°ï¼Œå°±åœ°è§£å†³
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)

        cond_norm = torch.norm(noise_pred_cond, dim=-1, keepdim=True)
        noise_norm = torch.norm(noise_pred, dim=-1, keepdim=True)
        noise_pred = noise_pred * (cond_norm / noise_norm)

        # Update latents
        # ç»“æœä¿ç•™åœ¨ GPU 6ï¼Œå‡†å¤‡ä¸‹ä¸€è½®æ‹‰å› GPU 1
        latents = latents.to("cuda:6")
        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]

    # --- Step 4: Decode (åœ¨ GPU 7) ---
    print("   [4/4] Decoding on GPU 7...")

    # ç§»åŠ¨åˆ° VAE æ‰€åœ¨çš„å¡
    latents = latents.to("cuda:7")

    # Unpack & Denormalize
    latents = helper_pipe._unpack_latents(latents, height, width, helper_pipe.vae_scale_factor)
    latents = latents.to(dtype)

    latents_mean = torch.tensor(vae.config.latents_mean).view(1, 16, 1, 1).to("cuda:7", dtype)
    latents_std = 1.0 / torch.tensor(vae.config.latents_std).view(1, 16, 1, 1).to("cuda:7", dtype)
    latents = latents / latents_std + latents_mean

    # Decode
    image = vae.decode(latents, return_dict=False)[0]

    # Post-process
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).float().numpy()
    image = (image * 255).round().astype("uint8")
    image = Image.fromarray(image[0])

torch.cuda.synchronize()
end_time = time.time()

# ==============================================================================
# 5. ä¿å­˜
# ==============================================================================
save_path = "output_v100_manual_512.png"
image.save(save_path)
print("-" * 30)
print(f"ğŸ‰ Success! Saved to {save_path}")
print(f"â±ï¸ Total Time: {end_time - start_time:.2f}s")
print("ğŸ’¡ éªŒè¯ï¼šæ­¤å›¾ç‰‡åº”è¯¥è‰²å½©æ­£å¸¸ï¼ˆéå…¨é»‘ï¼‰ï¼Œä¸”é€Ÿåº¦æ¯” Offload å¿«å¾—å¤šã€‚")